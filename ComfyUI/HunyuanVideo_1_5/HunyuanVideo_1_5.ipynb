{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **HunyuanVideo 1.5 for Text & Image to Video in ComfyUI**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "- Run the cell below to get a link (e.g. https://localhost:8188/) which you can use to launch the comfyUI interface.\n",
        "- Github project page: https://github.com/Tencent-Hunyuan/HunyuanVideo-1.5\n",
        "- Workflows: https://github.com/Isi-dev/Google-Colab_Notebooks/tree/main/ComfyUI/HunyuanVideo_1_5\n",
        "- Models source: (1) https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/tree/main/split_files (2) https://huggingface.co/jayn7/HunyuanVideo-1.5_T2V_480p-GGUF/tree/main/480p_distilled\n",
        "- Notebook source: https://github.com/Isi-dev/Google-Colab_Notebooks\n",
        "- Premium notebooks I highly recommend: https://isinse.gumroad.com/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xKrtSBCvRKvI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qVNbhfxXQe2V",
        "outputId": "d0e5dd96-1261-4679-c23c-2a77fa387aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.8.0\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision==0.23.0\n",
            "  Downloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch==2.8.0)\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.23.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp312-cp312-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n"
          ]
        }
      ],
      "source": [
        "# @markdown # üí•Prepare Environment\n",
        "\n",
        "!pip install torch==2.8.0 torchvision==0.23.0\n",
        "\n",
        "offload_models_for_low_VRAM = True # @param {type:\"boolean\"}\n",
        "include_manager = True # @param {type:\"boolean\"}\n",
        "\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.32.post1 triton==3.4 sageattention==1.0.6\n",
        "!pip install av spandrel albumentations onnx opencv-python onnxruntime\n",
        "!git clone https://github.com/comfyanonymous/ComfyUI\n",
        "!pip install -r /content/ComfyUI/requirements.txt\n",
        "clear_output()\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "if include_manager:\n",
        "    !git clone --branch com_man_3.37 https://github.com/Isi-dev/ComfyUI-Manager\n",
        "!git clone https://github.com/kijai/ComfyUI-KJNodes\n",
        "!git clone --branch forQwen https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_DeleteModelPassthrough_ExecutionFlowControl\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI-KJNodes\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_DeleteModelPassthrough_ExecutionFlowControl\n",
        "!pip install -r requirements.txt\n",
        "if include_manager:\n",
        "    %cd /content/ComfyUI/custom_nodes/ComfyUI-Manager\n",
        "    !pip install -r requirements.txt\n",
        "\n",
        "clear_output()\n",
        "\n",
        "\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "# sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "def install_apt_packages():\n",
        "    packages = ['aria2']\n",
        "\n",
        "    try:\n",
        "        # Run apt install silently (using -qq)\n",
        "        subprocess.run(\n",
        "            ['apt-get', '-y', 'install', '-qq'] + packages,\n",
        "            check=True,\n",
        "            capture_output=True\n",
        "        )\n",
        "        print(\"‚úì apt packages installed\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚úó Error installing apt packages: {e.stderr.decode().strip() or 'Unknown error'}\")\n",
        "\n",
        "\n",
        "print(\"Installing apt packages...\")\n",
        "install_apt_packages()\n",
        "\n",
        "def download_with_aria2c(link, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "\n",
        "    filename = link.split(\"/\")[-1]\n",
        "    command = f\"aria2c --console-log-level=error -c -x 16 -s 16 -k 1M {link} -d {folder} -o {filename}\"\n",
        "\n",
        "    print(\"Executing download command:\")\n",
        "    print(command)\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    get_ipython().system(command)\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "\n",
        "def download_civitai_model(civitai_link, civitai_token, folder=\"/content/ComfyUI/models/loras\"):\n",
        "    import os\n",
        "    import time\n",
        "\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        model_id = civitai_link.split(\"/models/\")[1].split(\"?\")[0]\n",
        "    except IndexError:\n",
        "        raise ValueError(\"Invalid Civitai URL format. Please use a link like: https://civitai.com/api/download/models/1523247?...\")\n",
        "\n",
        "    civitai_url = f\"https://civitai.com/api/download/models/{model_id}?type=Model&format=SafeTensor\"\n",
        "    if civitai_token:\n",
        "        civitai_url += f\"&token={civitai_token}\"\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"model_{timestamp}.safetensors\"\n",
        "\n",
        "    full_path = os.path.join(folder, filename)\n",
        "\n",
        "    download_command = f\"wget --max-redirect=10 --show-progress \\\"{civitai_url}\\\" -O \\\"{full_path}\\\"\"\n",
        "    print(\"Downloading from Civitai...\")\n",
        "\n",
        "    os.system(download_command)\n",
        "\n",
        "    local_path = os.path.join(folder, filename)\n",
        "    if os.path.exists(local_path) and os.path.getsize(local_path) > 0:\n",
        "        print(f\"LoRA downloaded successfully: {local_path}\")\n",
        "    else:\n",
        "        print(f\"‚ùå LoRA download failed or file is empty: {local_path}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "def download_lora(link, folder=\"/content/ComfyUI/models/loras\", civitai_token=None):\n",
        "    \"\"\"\n",
        "    Download a model file, automatically detecting if it's a Civitai link or huggingface download.\n",
        "\n",
        "    Args:\n",
        "        link: The download URL (either huggingface or Civitai)\n",
        "        folder: Destination folder for the download\n",
        "        civitai_token: Optional token for Civitai downloads (required if link is from Civitai)\n",
        "\n",
        "    Returns:\n",
        "        The filename of the downloaded model\n",
        "    \"\"\"\n",
        "    if \"civitai.com\" in link.lower():\n",
        "        if not civitai_token:\n",
        "            raise ValueError(\"Civitai token is required for Civitai downloads\")\n",
        "        return download_civitai_model(link, civitai_token, folder)\n",
        "    else:\n",
        "        return download_with_aria2c(link, folder)\n",
        "\n",
        "def model_download(url: str, dest_dir: str, filename: str = None, silent: bool = True) -> bool:\n",
        "    \"\"\"\n",
        "    Colab-optimized download with aria2c\n",
        "\n",
        "    Args:\n",
        "        url: Download URL\n",
        "        dest_dir: Target directory (will be created if needed)\n",
        "        filename: Optional output filename (defaults to URL filename)\n",
        "        silent: If True, suppresses all output (except errors)\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create destination directory\n",
        "        Path(dest_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Set filename if not specified\n",
        "        if filename is None:\n",
        "            filename = url.split('/')[-1].split('?')[0]  # Remove URL parameters\n",
        "\n",
        "        # Build command\n",
        "        cmd = [\n",
        "            'aria2c',\n",
        "            '--console-log-level=error',\n",
        "            '-c', '-x', '16', '-s', '16', '-k', '1M',\n",
        "            '-d', dest_dir,\n",
        "            '-o', filename,\n",
        "            url\n",
        "        ]\n",
        "\n",
        "        # Add silent flags if requested\n",
        "        if silent:\n",
        "            cmd.extend(['--summary-interval=0', '--quiet'])\n",
        "            print(f\"Downloading {filename}...\", end=' ', flush=True)\n",
        "\n",
        "        # Run download\n",
        "        result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "\n",
        "        if silent:\n",
        "            print(\"Done!\")\n",
        "        else:\n",
        "            print(f\"Downloaded {filename} to {dest_dir}\")\n",
        "        return filename\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        error = e.stderr.strip() or \"Unknown error\"\n",
        "        print(f\"\\nError downloading {filename}: {error}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "download_T2V_model = True # @param {type:\"boolean\"}\n",
        "hunyuan_T2V_download_url = \"https://huggingface.co/jayn7/HunyuanVideo-1.5_T2V_480p-GGUF/resolve/main/480p_distilled/hunyuanvideo1.5_480p_t2v_cfg_distilled-Q5_K_M.gguf\" # @param {\"type\":\"string\"}\n",
        "download_I2V_model = True # @param {type:\"boolean\"}\n",
        "hunyuan_I2V_download_url = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_480p_i2v_cfg_distilled_fp8_scaled.safetensors\" # @param {\"type\":\"string\"}\n",
        "download_upscale_model = True # @param {type:\"boolean\"}\n",
        "upscale_model_download_url = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/latent_upscale_models/hunyuanvideo15_latent_upsampler_720p.safetensors\" # @param {\"type\":\"string\"}\n",
        "download_speed_LoRA = True # @param {type:\"boolean\"}\n",
        "speed_LoRA_download_url = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/loras/hunyuanvideo1.5_t2v_480p_lightx2v_4step_lora_rank_32_bf16.safetensors\" # @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "if download_T2V_model:\n",
        "    model_download(hunyuan_T2V_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "if download_I2V_model:\n",
        "    model_download(hunyuan_I2V_download_url, \"/content/ComfyUI/models/diffusion_models\")\n",
        "if download_upscale_model:\n",
        "    model_download(upscale_model_download_url, \"/content/ComfyUI/models/latent_upscale_models\")\n",
        "if download_speed_LoRA:\n",
        "    model_download(speed_LoRA_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "\n",
        "text_encoder = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors\"\n",
        "text_encoder2 = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/byt5_small_glyphxl_fp16.safetensors\"\n",
        "vae = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/vae/hunyuanvideo15_vae_fp16.safetensors\"\n",
        "clip_vision = \"https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/clip_vision/sigclip_vision_patch14_384.safetensors\"\n",
        "\n",
        "model_download(text_encoder, \"/content/ComfyUI/models/text_encoders\")\n",
        "model_download(text_encoder2, \"/content/ComfyUI/models/text_encoders\")\n",
        "model_download(vae, \"/content/ComfyUI/models/vae\")\n",
        "model_download(clip_vision, \"/content/ComfyUI/models/clip_vision\")\n",
        "\n",
        "\n",
        "\n",
        "download_loRA1 = False # @param {type:\"boolean\"}\n",
        "lora1_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_2 = False # @param {type:\"boolean\"}\n",
        "lora_2_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_3 = False # @param {type:\"boolean\"}\n",
        "lora3_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "download_loRA_4 = False # @param {type:\"boolean\"}\n",
        "lora_4_download_url = \"Put your loRA here\"# @param {\"type\":\"string\"}\n",
        "\n",
        "token_if_civitai_url = \"\"# @param {\"type\":\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "lora1 = None\n",
        "if download_loRA1:\n",
        "    lora1 = download_lora(lora1_download_url, civitai_token=token_if_civitai_url)\n",
        "# Validate loRA file extension\n",
        "valid_extensions = {'.safetensors', '.ckpt', '.pt', '.pth', '.sft'}\n",
        "if lora1:\n",
        "    if not any(lora1.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"‚ùå Invalid LoRA format: {lora1}\")\n",
        "        lora1 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 1 downloaded succesfully!\")\n",
        "\n",
        "lora_2 = None\n",
        "if download_loRA_2:\n",
        "    lora_2 = download_lora(lora_2_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_2:\n",
        "    if not any(lora_2.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"‚ùå Invalid LoRA format: {lora_2}\")\n",
        "        lora_2 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 2 downloaded succesfully!\")\n",
        "\n",
        "lora_3 = None\n",
        "if download_loRA_3:\n",
        "    lora_3 = download_lora(lora3_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_3:\n",
        "    if not any(lora_3.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"‚ùå Invalid LoRA format: {lora_3}\")\n",
        "        lora_3 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 3 downloaded succesfully!\")\n",
        "\n",
        "lora_4 = None\n",
        "if download_loRA_4:\n",
        "    lora_4 = download_lora(lora_4_download_url, civitai_token=token_if_civitai_url)\n",
        "if lora_4:\n",
        "    if not any(lora_4.lower().endswith(ext) for ext in valid_extensions):\n",
        "        print(f\"‚ùå Invalid LoRA format: {lora_4}\")\n",
        "        lora_4 = None\n",
        "    else:\n",
        "        clear_output()\n",
        "        print(\"loRA 4 downloaded succesfully!\")\n",
        "\n",
        "\n",
        "# model = model_download(flux_model_download_url, \"/content/ComfyUI/models/unet\")\n",
        "# text_encoder = model_download(flux_text_encoder_download_url, \"/content/ComfyUI/models/clip\")\n",
        "# text_encoder2 = model_download(flux_text_encoder_download_url2, \"/content/ComfyUI/models/clip\")\n",
        "# vae = model_download(flux_vae_download_url, \"/content/ComfyUI/models/vae\")\n",
        "# lora = model_download(flux_uso_lora_download_url, \"/content/ComfyUI/models/loras\")\n",
        "# patch = model_download(flux_uso_patch_download_url, \"/content/ComfyUI/models/model_patches\")\n",
        "# clip_vision = model_download(clip_vision_download_url, \"/content/ComfyUI/models/clip_vision\")\n",
        "# turbo = model_download(speed_lora_download_url, \"/content/ComfyUI/models/loras\")\n",
        "\n",
        "\n",
        "\n",
        "use_cloudflare = False  # @param {type:\"boolean\"}\n",
        "use_interface_in_cell = False  # @param {type:\"boolean\"}\n",
        "use_ngrok = False  # @param {type:\"boolean\"}\n",
        "NGROK_AUTH_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# ---------------- NGROK IMPLEMENTATION ----------------\n",
        "if use_ngrok:\n",
        "    print(\"‚è≥ Setting up pyngrok tunnel...\")\n",
        "\n",
        "    # Install pyngrok\n",
        "    !pip install -q pyngrok\n",
        "\n",
        "    import time, socket, threading\n",
        "    from pyngrok import ngrok, conf\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    # Apply the ngrok authtoken (if provided)\n",
        "    if NGROK_AUTH_TOKEN.strip():\n",
        "        print(\"üîê Using provided ngrok authtoken...\")\n",
        "        conf.get_default().auth_token = NGROK_AUTH_TOKEN\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No authtoken provided. Please stop this execution and input your Authtoken.\")\n",
        "\n",
        "    def launch_ngrok(port):\n",
        "        # Wait for ComfyUI server to be ready before tunneling\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex((\"127.0.0.1\", port))\n",
        "            sock.close()\n",
        "            if result == 0:\n",
        "                break\n",
        "\n",
        "        print(\"\\nüöÄ ComfyUI loaded. Launching ngrok tunnel...\\n\")\n",
        "\n",
        "        # Close any previous tunnels just in case\n",
        "        ngrok.kill()\n",
        "\n",
        "        # Create the HTTP tunnel\n",
        "        public_url = ngrok.connect(addr=f\"http://127.0.0.1:{port}\", proto=\"http\")\n",
        "        clear_output()\n",
        "        print(\"‚úÖ ComfyUI is ready!\")\n",
        "        print(f\"üåê Access it here: {public_url.public_url}\")\n",
        "\n",
        "    threading.Thread(target=launch_ngrok, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if offload_models_for_low_VRAM:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "\n",
        "# ---------------- CLOUDFLARE IMPLEMENTATION ----------------\n",
        "elif use_cloudflare:\n",
        "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "    !dpkg -i cloudflared-linux-amd64.deb >/dev/null 2>&1\n",
        "\n",
        "    import subprocess, threading, time, socket\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    def cloudflare_thread(port):\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex(('127.0.0.1', port))\n",
        "            if result == 0:\n",
        "                break\n",
        "            sock.close()\n",
        "        print(\"\\nüöÄ ComfyUI loaded. Launching Cloudflare tunnel...\\n\")\n",
        "\n",
        "        p = subprocess.Popen([\"cloudflared\", \"tunnel\", \"--url\", f\"http://127.0.0.1:{port}\"],\n",
        "                             stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        for line in p.stderr:\n",
        "            l = line.decode()\n",
        "            if \"trycloudflare.com \" in l:\n",
        "                clear_output()\n",
        "                print(\"‚úÖ ComfyUI is ready!\")\n",
        "                print(\"üåê Access it here:\", l[l.find(\"http\"):], end='')\n",
        "                break\n",
        "\n",
        "    threading.Thread(target=cloudflare_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if offload_models_for_low_VRAM:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "# ---------------- IN-CELL INTERFACE ----------------\n",
        "elif use_interface_in_cell:\n",
        "    import threading, time, socket\n",
        "    from google.colab import output\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    def iframe_thread(port):\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex(('127.0.0.1', port))\n",
        "            if result == 0:\n",
        "                break\n",
        "            sock.close()\n",
        "        output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "        clear_output()\n",
        "        print(\"‚úÖ ComfyUI loaded inside the notebook.\")\n",
        "        print(\"To open it in a separate tab, click the link below:\")\n",
        "        output.serve_kernel_port_as_window(port)\n",
        "\n",
        "    threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if offload_models_for_low_VRAM:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "# ---------------- DEFAULT (COLAB NATIVE PORT FORWARDING) ----------------\n",
        "else:\n",
        "    import socket, time, threading\n",
        "    from google.colab import output\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    def colab_link_thread(port):\n",
        "        while True:\n",
        "            time.sleep(0.5)\n",
        "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "            result = sock.connect_ex(('127.0.0.1', port))\n",
        "            if result == 0:\n",
        "                break\n",
        "            sock.close()\n",
        "        clear_output()\n",
        "        print(\"‚úÖ ComfyUI is ready!\")\n",
        "        print(\"Click below to open the interface:\")\n",
        "        output.serve_kernel_port_as_window(port)\n",
        "\n",
        "    threading.Thread(target=colab_link_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "    if offload_models_for_low_VRAM:\n",
        "        !python main.py --cache-none --dont-print-server\n",
        "    else:\n",
        "        !python main.py --dont-print-server\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}